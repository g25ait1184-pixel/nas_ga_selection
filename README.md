Genetic Algorithmâ€“Based NAS 
=================================================================

#ğŸ“˜ Overview
This document provides a detailed explanation of the Genetic Algorithmâ€“based
Neural Architecture Search (NAS), covering Q1A (Roulette-Wheel Selection) and
Q2B (Weighted Fitness Function). This version is formatted cleanly for
Notepad++ viewing.

-----------------------------------------------------------------
#ğŸ”¹ â€” Roulette-Wheel Selection
-----------------------------------------------------------------

In the original NAS implementation, **Tournament Selection** was used.
This method is biased toward the fittest chromosomes in a small sampled group.

âœ” Advantage: Fast exploitation  
âœ– Disadvantage: Poor exploration, risk of premature convergence

To improve exploration, **Roulette-Wheel Selection** was implemented.

ğŸ“Œ Roulette-Wheel Formula:
    p_i = f_i / Î£ f_i

Where:
    p_i = probability of selecting chromosome i  
    f_i = fitness of chromosome i  

Meaning:
    â€¢ Higher fitness â†’ higher probability  
    â€¢ Lower fitness â†’ still has some chance  

This encourages genetic diversity and avoids stagnation.

-----------------------------------------------------------------
#ğŸ”¹ â€” Modified Fitness Function
-----------------------------------------------------------------

Original Fitness Function penalized only total parameters:

    fitness_orig = accuracy âˆ’ 0.01 Ã— (total_params / 1e6)

âš  Problem:
This treats convolution and fully-connected layers equally:
    - CONV layers â†’ high compute cost, moderate params
    - FC layers â†’ low compute, very high params

Thus the penalty is not realistic.

-----------------------------------------------------------------
#âœ” Modified Fitness Function (Conv vs FC Penalty)
-----------------------------------------------------------------

We split model parameters into:
    conv_params  = parameters of convolutional layers
    fc_params    = parameters of fully-connected layers

Normalized units (in millions):
    conv_M = conv_params / 1e6
    fc_M   = fc_params / 1e6

New weighted fitness:
    fitness_weighted = accuracy âˆ’ (w_conv Ã— conv_M + w_fc Ã— fc_M)

Weights used:
    w_conv = 1eâˆ’6  w_fc   = 5eâˆ’6
Justification:
    â€¢ Conv layers are compute-heavy â†’ mild penalty  
    â€¢ FC layers explode in size â†’ stronger penalty  
    â€¢ This promotes smaller, compute-friendly networks  

-----------------------------------------------------------------
#ğŸ“Š Experimental Results
-----------------------------------------------------------------

The following table summarizes the NAS results using both selection methods:

| Selection Method   | Accuracy | Original Fitness | Weighted Fitness | Parameters |
|--------------------|----------|------------------|------------------|------------|
| Tournament (Run 4) | 0.6770   | 0.6530           | 0.6769           | 2,398,250  |
| Roulette (Run 5)   | 0.6700   | 0.6617           | 0.6699           |   826,042  |

-----------------------------------------------------------------
#ğŸ§  Interpretation
-----------------------------------------------------------------

Tournament:
    âœ” Good accuracy
    âœ– Very large model (3Ã— more parameters)
    â†’ Lower fitness

Roulette:
    âœ” Slightly lower accuracy  
    âœ” Much fewer parameters  
    â†’ Higher final fitness

#ğŸ† Winner: Roulette-Wheel Selection

Roulette produced a more parameter-efficient architecture with better 
accuracyâ€“complexity trade-off.

-----------------------------------------------------------------
#âœ” Final Conclusion
-----------------------------------------------------------------

â€¢ Roulette selection improved diversity and avoided premature convergence  
â€¢ Weighted fitness accurately penalized FC-heavy models  
â€¢ The new NAS setup discovers smaller CNNs without losing accuracy  


